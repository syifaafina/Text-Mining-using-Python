{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextMining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-mrTVCfmbpX"
      },
      "source": [
        "# **Text Analytics & Text Mining** by Syifa Afina\n",
        "*Text Analytics* merupakan pendekatan dengan metodologi untuk mengekstrak pengetahuan dari data teks. Digunakan untuk kepentingan bisnis dan pengambilan keputusan lainnya.\n",
        "> \n",
        "Sedangkan *Text Mining* merupakan alat yang digunakan pada *Text Analytics*.\n",
        "\n",
        "\n",
        "## **Text Mining**\n",
        "*Text Mining* merupakan proses melihat pola dan insight baru yang berasal dari banyaknya data yang pada awalnya tidak terstruktur (*unstructured data*). Pengolahan *Text Mining* membutuhkan proses yang dinamakan *Pre-Processing Data*.\n",
        "\n",
        "## **Pre-Processing Data**\n",
        "*Pre-Processing Data* merupakan proses untuk mempersiapkan data mentah (*raw data*) sebelum dilakukannya proses lain. Pada umunya, *pre-processing data* dilakukan dengan cara mengeliminasi data yang tidak sesuai atau mengubah data menjadi bentuk yang lebih mudah diproses oleh sistem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMraqObsSHFt"
      },
      "source": [
        "Selain itu, praktikum kali ini akan membahas metode dalam *text mining* yaitu **Sentiment Analysis** dan **Topic Modelling**\n",
        "\n",
        "\n",
        "1.   **Sentiment Analysis**\n",
        "\n",
        "> Sentiment Analysis merupakan proses memahami dan mengelompokkan emosi (positif, negatif, dan netral) yang terdapat dalam tulisan menggunakan teknik Text Analytics. Data emosi tersebut dapat menjelaskan opini masyarakat mengenai produk, merek, layanan, politik, atau topik lainnya. Perusahaan, pemerintah, maupun bidang lainnya kemudian memanfaatkan data-data tersebut untuk membuat analisis pemasaran, review produk, umpan-balik produk, dan layanan masyarakat.\n",
        "\n",
        "2.   **Topic Modelling**\n",
        "\n",
        "> Topic Modelling merupakan pengelompokkan data teks berdasarkan suatu topik tertentu. Topic Modelling memiliki tujuan yang sama dengan classification tetapi menggunakan pendekatan berbeda. Topic Modelling tidak membutuhkan data berlabel (unsupervised learning) dan bekerja seperti clustering karena mengelompokkan data teks berdasarkan kemiripannya.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAnqVMgkRqy2"
      },
      "source": [
        "Praktikum kali ini menggunakan data tweet terkait Digital Currency sepanjang tahun 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6v1llWX1ybs"
      },
      "source": [
        "###**1. Install and Load Packages**\n",
        "\n",
        "- packages tweet-preprocessor: memudahkan untuk membersihkan, mengurai, atau memberi token pada tweet.\n",
        "- packages pyLDAvis: LDA (Latent Dirichlet Allocation) membantu dalam metode *Topic Modelling*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orcAloXKEtUi"
      },
      "source": [
        "# Install packages\n",
        "! pip install tweet-preprocessor\n",
        "! pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzBLSO587SAC"
      },
      "source": [
        "# Install pandas versi 1.3.0\n",
        "  # (untuk mempermudah proses transformasi data)\n",
        "! pip install pandas==1.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFNTlh7j0yW-"
      },
      "source": [
        "# Load packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocessor as p\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import wordcloud\n",
        "import nltk\n",
        "import warnings\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WPn6T5IYLO"
      },
      "source": [
        "# Import module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
        "from tqdm import tqdm\n",
        "from nltk import bigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TGl07g1GR_"
      },
      "source": [
        "# Set parameter\n",
        " # (untuk mengabaikan 'filter warnings')\n",
        "warnings.filterwarnings('ignore')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POd_N1Vx18Ft"
      },
      "source": [
        "### **2. Import Data**\n",
        "Dataset Digital Currency yang digunakan berasal dari github, sehingga import data langsung menggunakan link dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX4JNHdFmLbd"
      },
      "source": [
        "# Import data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/apriandito/digital-currency/main/data/tweet_digitalcurrency_text-sample.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9lNfxTM1wV0"
      },
      "source": [
        "# Lihat 5 baris pertama data\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfntSX4DopwQ"
      },
      "source": [
        "# Melihat ringkasan singkat dari dataset\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIXInlqooGaO"
      },
      "source": [
        "### **3. Text Preprocessing**\n",
        "Proses pengolahan data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PtaspkU8ei5"
      },
      "source": [
        "# Pilih kolom 'TEXT' saja\n",
        "tweet = df[['text']]\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmpAtGBFFGCG"
      },
      "source": [
        "#### **a. Transformation**\n",
        "Membersihkan dataset dari URLs, hashtags, mentions, RT & Fav, emoji, smileys, & number menggunakan packages tweet-preprocessor. Dilanjutkan dengan mengubah huruf pada dataset menjadi huruf kecil (lowercase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewjn6JMbDejZ"
      },
      "source": [
        "# Membuat fungsi transformasi tweet\n",
        " # dilanjutkan dengan mengubah huruf pada dataset menjadi lowercase\n",
        "\n",
        "def transform_tweet(row):\n",
        "  tweet = row['text']\n",
        "  tweet = p.clean(tweet)\n",
        "  tweet = str.lower(tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMa_w71GEaFS"
      },
      "source": [
        "# Mengaplikasikan fungsi transformasi dengan membuat kolom baru di dataset\n",
        "tweet['transformed'] = tweet.apply(transform_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()\n",
        "\n",
        "# kolom 'text' merupakan teks sebelum diubah\n",
        "# kolom 'transformed' merupakan teks setelah melalui proses transformasi (cleaning) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ZiliHSFSnH"
      },
      "source": [
        "#### **b. Tokenization**\n",
        "Memotong teks menjadi unit yang lebih kecil yang disebut 'token'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0l9wvMPFpNE"
      },
      "source": [
        "# Download Punkt\n",
        "# Punkt merupakan alat 'Tokenizer' yang membagi teks menjadi unit-unit yang lebih kecil\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJfyNe2VC6rN"
      },
      "source": [
        "# Membuat fungsi tokenization\n",
        "\n",
        "def tokenize_tweet(row):\n",
        "    tweet = row['transformed']\n",
        "    tokens = nltk.word_tokenize(tweet)\n",
        "    token_words = [w for w in tokens if w.isalpha()]\n",
        "    return token_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmjyoGi_rxN"
      },
      "source": [
        "# Mengaplikasikan fungsi tokenization dengan membuat kolom baru di dataset\n",
        "tweet['tokenized'] = tweet.apply(tokenize_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()\n",
        "\n",
        "# kolom 'text' merupakan teks sebelum diubah\n",
        "# kolom 'transformed' merupakan teks setelah melalui proses transformasi (cleaning)\n",
        "# kolom 'tokenized' merupakan teks setelah melalui proses tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJIYgfUq7gQC"
      },
      "source": [
        "#### **c. Stemming**\n",
        "Mengubah kata menjadi bentuk dasar umum (memotong/stem)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ZaFohE5unB"
      },
      "source": [
        "# Membuat fungsi stemming\n",
        "def stemming_tweet(row):\n",
        "    list = row['tokenized']\n",
        "    stemming_list = [PorterStemmer().stem(w) for w in list]\n",
        "    return(stemming_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pp8nUaS6GAU"
      },
      "source": [
        "# Mengaplikasikan fungsi stemming dengan membuat kolom baru di dataset\n",
        "tweet['stemmed'] = tweet.apply(stemming_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()\n",
        "\n",
        "# kolom 'text' merupakan teks sebelum diubah\n",
        "# kolom 'transformed' merupakan teks setelah melalui proses transformasi (cleaning)\n",
        "# kolom 'tokenized' merupakan teks setelah melalui proses tokenization\n",
        "# kolom 'stemmed' merupakan teks setelah melalui proses stemming"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtKXiR7YF9RZ"
      },
      "source": [
        "#### **d. Lemmatization**\n",
        "Mengubah kata menjadi bentuk dasar umum menggunakan analisis morfologi. Lemmatization biasanya lebih baik hasilnya dari Stemming. Stemming hanya memotong kata, sedangkan Lemmatization memotong kata dan menggunakan dictionary yang lebih lengkap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNKhzkZvF7mR"
      },
      "source": [
        "# Download Wordnet\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXSeJu9gGK5e"
      },
      "source": [
        "# Membuat fungsi lemmatization\n",
        "def lemmatize_tweet(row):\n",
        "    list = row['tokenized']\n",
        "    lemmatize_list = [WordNetLemmatizer().lemmatize(w, pos= 'v') for w in list]\n",
        "    return(lemmatize_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZm2bfvaGjyu"
      },
      "source": [
        "# Mengaplikasikan fungsi lemmatization dengan membuat kolom baru di dataset\n",
        "tweet['lemmatized'] = tweet.apply(lemmatize_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()\n",
        "\n",
        "# kolom 'text' merupakan teks sebelum diubah\n",
        "# kolom 'transformed' merupakan teks setelah melalui proses transformasi (cleaning)\n",
        "# kolom 'tokenized' merupakan teks setelah melalui proses tokenization\n",
        "# kolom 'stemmed' merupakan teks setelah melalui proses stemming\n",
        "# kolom 'lemmatized' merupakan teks setelah melalui proses lemmatization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeuh1xNYIzy5"
      },
      "source": [
        "#### **e. Stopword Removal**\n",
        "Menghapus kata yang tidak menambahkan arti pada kalimat.\n",
        "Contoh:\n",
        "> \n",
        "English: a, about, be, because, being, between, by, can't, do, during, from, have, he, she, him, her, it, is, i'm, you, too, they, this, etc.\n",
        "> \n",
        "Indonesian: ada, agar, apa, nah, pula, pun, itu, dan, wah, yang, atau, dll.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7XcHI7VIwjW"
      },
      "source": [
        "# Download stopword bahasa inggris\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words(\"english\"))     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvUidpdOBuBU"
      },
      "source": [
        "# Membuat fungsi stopword\n",
        "def stopword_tweet(row):\n",
        "    list = row['lemmatized']\n",
        "    stopword_list = [w for w in list if not w in stops]\n",
        "    return(stopword_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPei0h4uCEh5"
      },
      "source": [
        "# Mengaplikasikan fungsi Stopword dengan membuat kolom baru di dataset\n",
        "tweet['stopword'] = tweet.apply(stopword_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()\n",
        "\n",
        "# kolom 'text' merupakan teks sebelum diubah\n",
        "# kolom 'transformed' merupakan teks setelah melalui proses transformasi (cleaning)\n",
        "# kolom 'tokenized' merupakan teks setelah melalui proses tokenization\n",
        "# kolom 'stemmed' merupakan teks setelah melalui proses stemming\n",
        "# kolom 'lemmatized' merupakan teks setelah melalui proses lemmatization\n",
        "# kolom 'stopword' merupakan teks setelah melalui proses stopword removal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkl9x-LI4HL"
      },
      "source": [
        "#### **f. Rejoin Token**\n",
        "Menggabungkan token / kata-kata yang telah sampai proses stopword menjadi kalimat utuh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGzqcONlCHYo"
      },
      "source": [
        "# Membuat fungsi rejoin untuk mengembalikan sebagai kalimat utuh\n",
        "def rejoin_tweet(row):\n",
        "    list = row['stopword']\n",
        "    joined_words = ( \" \".join(list))\n",
        "    return joined_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GkNjo6bCc36"
      },
      "source": [
        "# Mengaplikasikan fungsi rejoin dengan membuat kolom baru di dataset\n",
        "tweet['final'] = tweet.apply(rejoin_tweet, axis=1)\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet.head()\n",
        "\n",
        "# kolom 'text' merupakan teks sebelum diubah\n",
        "# kolom 'transformed' merupakan teks setelah melalui proses transformasi (cleaning)\n",
        "# kolom 'tokenized' merupakan teks setelah melalui proses tokenization\n",
        "# kolom 'stemmed' merupakan teks setelah melalui proses stemming\n",
        "# kolom 'lemmatized' merupakan teks setelah melalui proses lemmatization\n",
        "# kolom 'stopword' merupakan teks setelah melalui proses stopword removal\n",
        "# kolom 'final' merupakan teks setelah melalui proses rejoin token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OkxLRK7JwRZ"
      },
      "source": [
        "# Final tweet yang sudah di proses\n",
        " # mengubah nama kolom \"final\" menjadi \"text\" untuk ditampilkan dengan variabel tweet_clean\n",
        "\n",
        "tweet_clean = tweet[['final']]\n",
        "tweet_clean = tweet_clean.rename(columns={'final': 'text'})\n",
        "\n",
        "# Lihat 5 baris pertama data\n",
        "tweet_clean.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_YvObORoOm8"
      },
      "source": [
        "### **4. WordCloud**\n",
        "WordCloud merupakan bagian dari Content Description untuk melihat kemunculan dari kata-kata. Kata yang ukurannya paling besar yang muncul merupakan kata yang paling banyak kuantitasnya dalam dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmiFeAwzoOK2"
      },
      "source": [
        "# Visualisasi Word Cloud\n",
        "text_wordcloud = \" \".join(tweet for tweet in tweet_clean.text)\n",
        "\n",
        "cloud = WordCloud(background_color='white').generate(text_wordcloud)\n",
        "\n",
        "plt.figure(figsize=(10, 10), facecolor=None)\n",
        "plt.imshow(cloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VylHW2_Um_eo"
      },
      "source": [
        "### **5. Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqUKHLCim63Q"
      },
      "source": [
        "# Download corpus untuk sentiment analysis\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHhRbz0nJ6J"
      },
      "source": [
        "# Sentiment Analysis\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "listy = [] \n",
        "for index, row in tweet_clean.iterrows():\n",
        "  ss = sid.polarity_scores(row['text'])\n",
        "  listy.append(ss)\n",
        "  \n",
        "se = pd.Series(listy)\n",
        "tweet_clean['polarity'] = se.values\n",
        "display(tweet_clean.head(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7V6DNsMf9p"
      },
      "source": [
        "# Visualisasi Pie Chart\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "sizes  = [ss['neg'], ss['neu'], ss['pos']]\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvLdU2GGoY_d"
      },
      "source": [
        "### **6. Topic Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSyZiXaiNI3P"
      },
      "source": [
        "# clone tambahan library dari github\n",
        "! git clone https://github.com/machine-learning-ss/tm\n",
        "\n",
        "# Set Data Directory\n",
        "os.chdir('tm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sbfDlWNJUp"
      },
      "source": [
        "# Melihat total loaded tweets\n",
        "\n",
        "import MyLib as TS\n",
        "\n",
        "Tweets = tweet_clean['text']\n",
        "print('Total loaded tweets = {0}'.format(len(Tweets)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPBZxuW0NYoH"
      },
      "source": [
        "# Menentukan banyaknya jumlah topik dan kata-kata yang akan diambil\n",
        "n_topics = 4\n",
        "top_topics = 4\n",
        "top_words = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mznEgLNvK2"
      },
      "source": [
        "# Feature Extraction\n",
        "count_vector = CountVectorizer(token_pattern = r'\\b[a-zA-Z]{3,}\\b') \n",
        "dtm_tf = count_vector.fit_transform(Tweets)\n",
        "tf_terms = count_vector.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NgY3twNwBB"
      },
      "source": [
        "# Fungsi untuk mencari topic\n",
        "lda_tf = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
        "\n",
        "# Menampilkan Topik\n",
        "vsm_topics = lda_tf.transform(dtm_tf); doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
        "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
        "plt.hist(np.array(doc_topic), alpha=0.5); plt.show()\n",
        "print('Printing top {0} Topics, with top {1} Words:'.format(top_topics, top_words))\n",
        "TS.print_Topics(lda_tf, tf_terms, top_topics, top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOcvEGgENy0f"
      },
      "source": [
        "# Visualisasi Topic Secara Interaktif\n",
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, count_vector) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}